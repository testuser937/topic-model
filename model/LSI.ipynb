{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce01ac6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'artm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SULEYM~1\\AppData\\Local\\Temp/ipykernel_16320/2232384436.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0martm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'artm'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ced0711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python39\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#1introduction\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "#data = pd.read_csv('article_all.csv', error_bad_lines=False, encoding=\"cp1251\");\n",
    "# data = pd.read_csv('lemmatized.csv', error_bad_lines=False, encoding=\"cp1251\");\n",
    "data = pd.read_csv('cleaned_stopwords.csv', encoding=\"cp1251\", sep='\\t');\n",
    "data_text = data[['out']]\n",
    "#documents = data_text\n",
    "#data_text['index'] = data_text.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e7b5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['изучение', 'энцевир', 'пря', 'экспресссхеме', 'нпо', 'микрогеннпо', 'вирион', 'гиск', 'заболеваемость', 'клещевои', 'энцефалит', 'продолжать', 'оставаться', 'актуальныи', 'здравоохранение', 'основныи', 'средство', 'защита', 'клещевои', 'энцефалит']\n"
     ]
    }
   ],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(data_text['out'])\n",
    "\n",
    "print (data_words[0][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68cfe468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 3), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 6), (9, 2), (10, 1), (11, 1), (12, 1), (13, 2), (14, 8), (15, 2), (16, 1), (17, 1), (18, 1), (19, 2)]\n",
      "адъювантом\n"
     ]
    }
   ],
   "source": [
    "#id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "corpus = []\n",
    "for text in data_words:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "\n",
    "print (corpus[0][0:20])\n",
    "\n",
    "word = id2word[[0][:1][0]]\n",
    "print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6080a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.242*\"клетка\" + 0.197*\"группа\" + 0.174*\"уровень\" + 0.161*\"система\" + 0.161*\"исследование\" + 0.147*\"больных\" + 0.136*\"активность\" + 0.135*\"метод\" + 0.134*\"являться\" + 0.134*\"кровь\"'), (1, '0.260*\"клетка\" + 0.174*\"больных\" + -0.146*\"информация\" + 0.146*\"кровь\" + -0.145*\"год\" + -0.144*\"система\" + 0.136*\"активность\" + -0.134*\"информационныи\" + 0.133*\"группа\" + -0.119*\"новыи\"'), (2, '-0.317*\"год\" + 0.271*\"система\" + -0.198*\"больных\" + 0.186*\"клетка\" + 0.171*\"модель\" + -0.145*\"группа\" + 0.143*\"сеть\" + -0.136*\"ребенок\" + -0.119*\"воина\" + 0.113*\"информационныи\"'), (3, '0.428*\"клетка\" + -0.307*\"больных\" + 0.210*\"мышь\" + -0.185*\"группа\" + 0.150*\"животное\" + -0.142*\"система\" + -0.142*\"ребенок\" + -0.138*\"уровень\" + -0.127*\"пациент\" + -0.110*\"лечение\"'), (4, '0.398*\"информационныи\" + 0.307*\"информация\" + 0.183*\"социальныи\" + 0.154*\"общество\" + -0.149*\"здание\" + -0.145*\"сооружение\" + 0.139*\"библиотека\" + -0.118*\"конструкция\" + 0.116*\"коммуникация\" + 0.113*\"человек\"'), (5, '-0.566*\"агент\" + -0.376*\"сеть\" + -0.306*\"социальныи\" + 0.222*\"библиотека\" + -0.179*\"модель\" + -0.165*\"влияние\" + -0.144*\"мнение\" + 0.132*\"информационныи\" + 0.122*\"информация\" + 0.114*\"система\"')]\n",
      "0.48200101099119497\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lsi_model = gensim.models.lsimodel.LsiModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=6,\n",
    "    chunksize=2000,\n",
    "    #power_iters = 3\n",
    "    onepass = True\n",
    ")\n",
    "print(lsi_model.print_topics())\n",
    "coherence_model_lda = CoherenceModel(model=lsi_model, texts=data_words, dictionary=id2word, coherence='c_v')\n",
    "print(coherence_model_lda.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00bfcd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.242*\"клетка\" + 0.197*\"группа\" + 0.174*\"уровень\" + 0.161*\"система\" + 0.161*\"исследование\" + 0.147*\"больных\" + 0.136*\"активность\" + 0.135*\"метод\" + 0.134*\"являться\" + 0.134*\"кровь\"'),\n",
       " (1,\n",
       "  '0.260*\"клетка\" + 0.174*\"больных\" + -0.146*\"информация\" + 0.146*\"кровь\" + -0.145*\"год\" + -0.144*\"система\" + 0.136*\"активность\" + -0.134*\"информационныи\" + 0.133*\"группа\" + -0.119*\"новыи\"'),\n",
       " (2,\n",
       "  '-0.317*\"год\" + 0.271*\"система\" + -0.198*\"больных\" + 0.186*\"клетка\" + 0.171*\"модель\" + -0.145*\"группа\" + 0.143*\"сеть\" + -0.136*\"ребенок\" + -0.119*\"воина\" + 0.113*\"информационныи\"'),\n",
       " (3,\n",
       "  '0.428*\"клетка\" + -0.307*\"больных\" + 0.210*\"мышь\" + -0.185*\"группа\" + 0.150*\"животное\" + -0.142*\"система\" + -0.142*\"ребенок\" + -0.138*\"уровень\" + -0.127*\"пациент\" + -0.110*\"лечение\"'),\n",
       " (4,\n",
       "  '0.398*\"информационныи\" + 0.307*\"информация\" + 0.183*\"социальныи\" + 0.154*\"общество\" + -0.149*\"здание\" + -0.145*\"сооружение\" + 0.139*\"библиотека\" + -0.118*\"конструкция\" + 0.116*\"коммуникация\" + 0.113*\"человек\"'),\n",
       " (5,\n",
       "  '-0.566*\"агент\" + -0.376*\"сеть\" + -0.306*\"социальныи\" + 0.222*\"библиотека\" + -0.179*\"модель\" + -0.165*\"влияние\" + -0.144*\"мнение\" + 0.132*\"информационныи\" + 0.122*\"информация\" + 0.114*\"система\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0403014a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 topics - 0.48200101099119497\n",
      "90.19460248947144 seconds \n",
      "\n",
      "7 topics - 0.47451054492360445\n",
      "94.84637975692749 seconds \n",
      "\n",
      "8 topics - 0.466498799827748\n",
      "97.44681191444397 seconds \n",
      "\n",
      "9 topics - 0.45235262832497747\n",
      "98.19161629676819 seconds \n",
      "\n",
      "10 topics - 0.44026190870348625\n",
      "100.68964719772339 seconds \n",
      "\n",
      "Wall time: 8min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "topic_min = 6\n",
    "topic_max = 11\n",
    "topics = range(topic_min, topic_max)\n",
    "for n in topics:\n",
    "    start_time = time.time()\n",
    "    lsi_model = gensim.models.lsimodel.LsiModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=n,\n",
    "        chunksize=2000,\n",
    "        onepass = True\n",
    "    )\n",
    "    \n",
    "    lsi_model.print_topics()\n",
    "    coherence_model_lda = CoherenceModel(model=lsi_model, texts=data_words, dictionary=id2word, coherence='c_v')\n",
    "    coherence = coherence_model_lda.get_coherence()\n",
    "    print(f'{n} topics - {coherence}')\n",
    "    print(\"%s seconds\" % (time.time() - start_time), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8abc208",
   "metadata": {},
   "source": [
    "# TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f90f4b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold = 100)\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram[bigram[doc]] for doc in texts]\n",
    "\n",
    "data_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec6a4a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 3), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 6), (9, 2), (10, 1), (11, 1), (12, 1), (13, 2), (14, 5), (15, 2), (16, 1), (17, 1), (18, 1), (19, 2)]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)    \n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[0][0:20])\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words = []\n",
    "words_missing_in_tfidf = []\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = []\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_values_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words + words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]\n",
    "    \n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f36513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 topics - 0.5305455228940321\n",
      "106.51702642440796 seconds \n",
      "\n",
      "7 topics - 0.49638669587890344\n",
      "102.20265293121338 seconds \n",
      "\n",
      "8 topics - 0.4831210020185078\n",
      "108.0712161064148 seconds \n",
      "\n",
      "9 topics - 0.4435982764354449\n",
      "112.06012225151062 seconds \n",
      "\n",
      "10 topics - 0.4466676574399281\n",
      "110.78365349769592 seconds \n",
      "\n",
      "Wall time: 8min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "topic_min = 6\n",
    "topic_max = 11\n",
    "topics = range(topic_min, topic_max)\n",
    "for n in topics:\n",
    "    start_time = time.time()\n",
    "    lsi_model = gensim.models.lsimodel.LsiModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=n,\n",
    "        chunksize=2000,\n",
    "        onepass = True\n",
    "    )\n",
    "    \n",
    "    lsi_model.print_topics()\n",
    "    coherence_model_lda = CoherenceModel(model=lsi_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "    coherence = coherence_model_lda.get_coherence()\n",
    "    print(f'{n} topics - {coherence}')\n",
    "    print(\"%s seconds\" % (time.time() - start_time), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
